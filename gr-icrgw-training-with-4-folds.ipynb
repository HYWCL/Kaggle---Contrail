{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":51753,"databundleVersionId":5692552,"sourceType":"competition"},{"sourceId":1323925,"sourceType":"datasetVersion","datasetId":767903},{"sourceId":4125181,"sourceType":"datasetVersion","datasetId":2437947},{"sourceId":5824224,"sourceType":"datasetVersion","datasetId":3347112},{"sourceId":5848162,"sourceType":"datasetVersion","datasetId":3362727},{"sourceId":6109993,"sourceType":"datasetVersion","datasetId":2437951},{"sourceId":6229868,"sourceType":"datasetVersion","datasetId":3578536},{"sourceId":6229922,"sourceType":"datasetVersion","datasetId":3439528,"isSourceIdPinned":true}],"dockerImageVersionId":30498,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Summary\n#### [<u>Initial version</u>](https://www.kaggle.com/code/egortrushin/gr-icrgw-pytorch-lightning-baseline-unet-resnest)\n- Baseline written using Pytorch Lightning\n- resnest26d as encoder\n- Dataset is taken from: https://www.kaggle.com/datasets/shashwatraman/contrails-images-ash-color\n\n#### [<u>Improved version</u>](https://www.kaggle.com/code/egortrushin/gr-icrgw-pl-pipeline-improved)\n- Option to change image size\n- Mixed precision training (only useful with T4x2, on P100 this slows down training). This helps to use GPU memory more efficiently\n- Training using 2 GPUs - with 2 GPUs we have more memory and higher speed\n- Other numerous small changes\n\n#### <u>Present version</u>\n- Training with 4-folds\n- LR scheduler: cosine with warmup\n- Use of CSVLogger with consequent visualization of the optimization process. Since I train without internet, I am limited to *local* CSVLogger or TensorBoardLogger. Alternatively you can train with internet and WanddbLogger.\n- Submission part is rewritten to make it cleaner and to allow easy work with multi-fold models","metadata":{}},{"cell_type":"markdown","source":"### Training part","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append(\"../input/pretrained-models-pytorch\")\nsys.path.append(\"../input/efficientnet-pytorch\")\nsys.path.append(\"/kaggle/input/smp-github/segmentation_models.pytorch-master\")\nsys.path.append(\"/kaggle/input/timm-pretrained-resnest/resnest/\")\nimport segmentation_models_pytorch as smp\nimport timm","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-08-01T09:17:20.877497Z","iopub.execute_input":"2023-08-01T09:17:20.878054Z","iopub.status.idle":"2023-08-01T09:17:20.887886Z","shell.execute_reply.started":"2023-08-01T09:17:20.878011Z","shell.execute_reply":"2023-08-01T09:17:20.886676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir -p /root/.cache/torch/hub/checkpoints/\n!cp /kaggle/input/timm-pretrained-resnest/resnest/gluon_resnest26-50eb607c.pth /root/.cache/torch/hub/checkpoints/gluon_resnest26-50eb607c.pth","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-08-01T09:17:20.890118Z","iopub.execute_input":"2023-08-01T09:17:20.890522Z","iopub.status.idle":"2023-08-01T09:17:24.074109Z","shell.execute_reply.started":"2023-08-01T09:17:20.890491Z","shell.execute_reply":"2023-08-01T09:17:24.072627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport numpy as np\nimport pandas as pd\nimport os\nimport random\nimport sys\nimport albumentations as A\n\nfrom tqdm.notebook import tqdm\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\nset_seed()","metadata":{"execution":{"iopub.status.busy":"2023-08-01T09:17:24.077394Z","iopub.execute_input":"2023-08-01T09:17:24.078695Z","iopub.status.idle":"2023-08-01T09:17:24.100645Z","shell.execute_reply.started":"2023-08-01T09:17:24.078630Z","shell.execute_reply":"2023-08-01T09:17:24.099450Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile config.yaml\n\ndata_path: \"/kaggle/input/contrails-images-ash-color\"\ndata_path2: \"/kaggle/input/contrails-frame6/\"\noutput_dir: \"models\"\n\nfolds:\n    n_splits: 4\n    random_state: 42\ntrain_folds: [0]\n    \nseed: 42\n\ntrain_bs: 8\nvalid_bs: 8\nworkers: 2\n\nprogress_bar_refresh_rate: 1\n\nearly_stop:\n    monitor: \"val_loss\"\n    mode: \"min\"\n    patience: 8\n    verbose: 1\n\ntrainer:\n    max_epochs: 20\n    min_epochs: 20\n    enable_progress_bar: True\n    precision: \"16-mixed\"\n    devices: 2\n\nmodel:\n    seg_model: \"Unet\"\n    encoder_name: \"tu-maxvit_rmlp_base_rw_384\"\n    loss_smooth: 1.0\n    image_size: 384\n    optimizer_params:\n        lr: 0.000001\n        weight_decay: 0.0\n    scheduler:\n        name: \"OneCycleLR\"\n        params:\n            OneCycleLR:\n                max_lr: 0.00004\n                epochs: 20\n                steps_per_epoch: 1122","metadata":{"execution":{"iopub.status.busy":"2023-08-01T09:17:24.107442Z","iopub.execute_input":"2023-08-01T09:17:24.107889Z","iopub.status.idle":"2023-08-01T09:17:24.309080Z","shell.execute_reply.started":"2023-08-01T09:17:24.107853Z","shell.execute_reply":"2023-08-01T09:17:24.307849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dataset\n\nimport torch\nimport numpy as np\nimport torchvision.transforms as T\n\nclass ContrailsDataset(torch.utils.data.Dataset):\n    def __init__(self, df, image_size=256, train=True):\n\n        self.df = df\n        self.trn = train\n        self.normalize_image = T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n        self.image_size = image_size\n        if image_size != 256:\n            self.resize_image = T.transforms.Resize(image_size)\n\n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        con_path = row.path\n        if con_path.find(\"/kaggle/input/contrails-frame6\"):\n            con = np.load(str(con_path[:-5] + con_path[-4:]))\n        else:\n            con = np.load(str(con_path))\n\n        img = con[..., :-1]\n        label = con[..., -1]\n        \n        if self.trn == True:\n            augmentation = self.get_training_augmentation()(image=img, label=label)\n            img, label = augmentation['image'], augmentation['label']\n\n        img = torch.tensor(np.reshape(img, (256, 256, 3))).to(torch.float32).permute(2, 0, 1)\n        label = torch.tensor(label)\n\n        if self.image_size != 256:\n            img = self.resize_image(img)\n\n        img = self.normalize_image(img)\n\n        return img.float(), label.float()\n\n    def __len__(self):\n        return len(self.df)\n    \n    def get_training_augmentation(self):\n        train_transform = A.Compose([\n            A.OneOf([A.HorizontalFlip(p=1),\n                    A.VerticalFlip(p=1),\n                    A.RandomRotate90(p=1)\n                    ],p=0.2)])\n        return train_transform","metadata":{"execution":{"iopub.status.busy":"2023-08-01T09:17:24.311911Z","iopub.execute_input":"2023-08-01T09:17:24.313655Z","iopub.status.idle":"2023-08-01T09:17:24.334236Z","shell.execute_reply.started":"2023-08-01T09:17:24.313620Z","shell.execute_reply":"2023-08-01T09:17:24.333115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lightning module\n\nimport torch\nimport pytorch_lightning as pl\nimport segmentation_models_pytorch as smp\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau, OneCycleLR\nfrom torch.optim import AdamW\nimport torch.nn as nn\nfrom torchmetrics.functional import dice\nfrom transformers import get_cosine_with_hard_restarts_schedule_with_warmup\n\nseg_models = {\n    \"Unet\": smp.Unet,\n    \"Unet++\": smp.UnetPlusPlus,\n    \"MAnet\": smp.MAnet,\n    \"Linknet\": smp.Linknet,\n    \"FPN\": smp.FPN,\n    \"PSPNet\": smp.PSPNet,\n    \"PAN\": smp.PAN,\n    \"DeepLabV3\": smp.DeepLabV3,\n    \"DeepLabV3+\": smp.DeepLabV3Plus,\n}\n\n\nclass LightningModule(pl.LightningModule):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.model = model = seg_models[config[\"seg_model\"]](\n            encoder_name=config[\"encoder_name\"],\n            encoder_weights=\"imagenet\",\n            in_channels=3,\n            classes=1,\n            activation=None,\n        )\n        self.loss_module = smp.losses.DiceLoss(mode=\"binary\", smooth=config[\"loss_smooth\"])\n        self.val_step_outputs = []\n        self.val_step_labels = []\n\n    def forward(self, batch):\n        imgs = batch\n        preds = self.model(imgs)\n        return preds\n\n    def configure_optimizers(self):\n        optimizer = AdamW(self.parameters(), **self.config[\"optimizer_params\"])\n\n        if self.config[\"scheduler\"][\"name\"] == \"CosineAnnealingLR\":\n            scheduler = CosineAnnealingLR(\n                optimizer,\n                **self.config[\"scheduler\"][\"params\"][\"CosineAnnealingLR\"],\n            )\n            lr_scheduler_dict = {\"scheduler\": scheduler, \"interval\": \"step\"}\n            return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler_dict}\n        elif self.config[\"scheduler\"][\"name\"] == \"ReduceLROnPlateau\":\n            scheduler = ReduceLROnPlateau(\n                optimizer,\n                **self.config[\"scheduler\"][\"params\"][\"ReduceLROnPlateau\"],\n            )\n            lr_scheduler = {\"scheduler\": scheduler, \"monitor\": \"val_loss\"}\n            return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler}\n        elif self.config[\"scheduler\"][\"name\"] == \"cosine_with_hard_restarts_schedule_with_warmup\":\n            scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(\n                optimizer,\n                **self.config[\"scheduler\"][\"params\"][self.config[\"scheduler\"][\"name\"]],\n            )\n            lr_scheduler_dict = {\"scheduler\": scheduler, \"interval\": \"step\"}\n            return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler_dict}\n        elif self.config[\"scheduler\"][\"name\"] == \"OneCycleLR\":\n            scheduler = OneCycleLR(\n                optimizer,\n                **self.config[\"scheduler\"][\"params\"][self.config[\"scheduler\"][\"name\"]],\n            )\n            lr_scheduler_dict = {\"scheduler\": scheduler, \"interval\": \"step\"}\n            return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler_dict}\n\n    def training_step(self, batch, batch_idx):\n        imgs, labels = batch\n        preds = self.model(imgs)\n        if self.config[\"image_size\"] != 256:\n            preds = torch.nn.functional.interpolate(preds, size=256, mode='bilinear')\n        loss = self.loss_module(preds, labels)\n        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, batch_size=16)\n\n        for param_group in self.trainer.optimizers[0].param_groups:\n            lr = param_group[\"lr\"]\n        self.log(\"lr\", lr, on_step=True, on_epoch=False, prog_bar=True)\n\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        imgs, labels = batch\n        preds = self.model(imgs)\n        if self.config[\"image_size\"] != 256:\n            preds = torch.nn.functional.interpolate(preds, size=256, mode='bilinear')\n        loss = self.loss_module(preds, labels)\n        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n        self.val_step_outputs.append(preds)\n        self.val_step_labels.append(labels)\n\n    def on_validation_epoch_end(self):\n        all_preds = torch.cat(self.val_step_outputs)\n        all_labels = torch.cat(self.val_step_labels)\n        all_preds = torch.sigmoid(all_preds)\n        self.val_step_outputs.clear()\n        self.val_step_labels.clear()\n        val_dice = dice(all_preds, all_labels.long())\n        self.log(\"val_dice\", val_dice, on_step=False, on_epoch=True, prog_bar=True)\n        if self.trainer.global_rank == 0:\n            print(f\"\\nEpoch: {self.current_epoch}\", flush=True)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-08-01T09:17:24.339749Z","iopub.execute_input":"2023-08-01T09:17:24.342434Z","iopub.status.idle":"2023-08-01T09:17:24.381784Z","shell.execute_reply.started":"2023-08-01T09:17:24.342397Z","shell.execute_reply":"2023-08-01T09:17:24.380743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lightning module\n\nimport torch\nimport pytorch_lightning as pl\nimport segmentation_models_pytorch as smp\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau, OneCycleLR\nfrom torch.optim import AdamW\nimport torch.nn as nn\nfrom torchmetrics.functional import dice\nfrom transformers import get_cosine_with_hard_restarts_schedule_with_warmup\n\nseg_models = {\n    \"Unet\": smp.Unet,\n    \"Unet++\": smp.UnetPlusPlus,\n    \"MAnet\": smp.MAnet,\n    \"Linknet\": smp.Linknet,\n    \"FPN\": smp.FPN,\n    \"PSPNet\": smp.PSPNet,\n    \"PAN\": smp.PAN,\n    \"DeepLabV3\": smp.DeepLabV3,\n    \"DeepLabV3+\": smp.DeepLabV3Plus,\n}\n\n\nclass LightningModule(pl.LightningModule):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.model = model = seg_models[config[\"seg_model\"]](\n            encoder_name=config[\"encoder_name\"],\n            encoder_weights=\"imagenet\",\n            in_channels=3,\n            classes=1,\n            activation=None,\n        )\n        self.loss_module = smp.losses.DiceLoss(mode=\"binary\", smooth=config[\"loss_smooth\"])\n        self.val_step_outputs = []\n        self.val_step_labels = []\n\n    def forward(self, batch):\n        imgs = batch\n        preds = self.model(imgs)\n        return preds\n\n    def configure_optimizers(self):\n        optimizer = AdamW(self.parameters(), **self.config[\"optimizer_params\"])\n\n        if self.config[\"scheduler\"][\"name\"] == \"CosineAnnealingLR\":\n            scheduler = CosineAnnealingLR(\n                optimizer,\n                **self.config[\"scheduler\"][\"params\"][\"CosineAnnealingLR\"],\n            )\n            lr_scheduler_dict = {\"scheduler\": scheduler, \"interval\": \"step\"}\n            return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler_dict}\n        elif self.config[\"scheduler\"][\"name\"] == \"ReduceLROnPlateau\":\n            scheduler = ReduceLROnPlateau(\n                optimizer,\n                **self.config[\"scheduler\"][\"params\"][\"ReduceLROnPlateau\"],\n            )\n            lr_scheduler = {\"scheduler\": scheduler, \"monitor\": \"val_loss\"}\n            return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler}\n        elif self.config[\"scheduler\"][\"name\"] == \"cosine_with_hard_restarts_schedule_with_warmup\":\n            scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(\n                optimizer,\n                **self.config[\"scheduler\"][\"params\"][self.config[\"scheduler\"][\"name\"]],\n            )\n            lr_scheduler_dict = {\"scheduler\": scheduler, \"interval\": \"step\"}\n            return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler_dict}\n        elif self.config[\"scheduler\"][\"name\"] == \"OneCycleLR\":\n            scheduler = OneCycleLR(\n                optimizer,\n                **self.config[\"scheduler\"][\"params\"][self.config[\"scheduler\"][\"name\"]],\n            )\n            lr_scheduler_dict = {\"scheduler\": scheduler, \"interval\": \"step\"}\n            return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler_dict}\n\n    def training_step(self, batch, batch_idx):\n        imgs, labels = batch\n        preds = self.model(imgs)\n        if self.config[\"image_size\"] != 256:\n            preds = torch.nn.functional.interpolate(preds, size=256, mode='bilinear')\n        loss = self.loss_module(preds, labels)\n        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, batch_size=16)\n\n        for param_group in self.trainer.optimizers[0].param_groups:\n            lr = param_group[\"lr\"]\n        self.log(\"lr\", lr, on_step=True, on_epoch=False, prog_bar=True)\n\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        imgs, labels = batch\n        preds = self.model(imgs)\n        if self.config[\"image_size\"] != 256:\n            preds = torch.nn.functional.interpolate(preds, size=256, mode='bilinear')\n        loss = self.loss_module(preds, labels)\n        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n        self.val_step_outputs.append(preds)\n        self.val_step_labels.append(labels)\n\n    def on_validation_epoch_end(self):\n        all_preds = torch.cat(self.val_step_outputs)\n        all_labels = torch.cat(self.val_step_labels)\n        all_preds = torch.sigmoid(all_preds)\n        self.val_step_outputs.clear()\n        self.val_step_labels.clear()\n        val_dice = dice(all_preds, all_labels.long())\n        self.log(\"val_dice\", val_dice, on_step=False, on_epoch=True, prog_bar=True)\n        if self.trainer.global_rank == 0:\n            print(f\"\\nEpoch: {self.current_epoch}\", flush=True)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-08-01T09:17:24.387032Z","iopub.execute_input":"2023-08-01T09:17:24.389822Z","iopub.status.idle":"2023-08-01T09:17:24.422446Z","shell.execute_reply.started":"2023-08-01T09:17:24.389789Z","shell.execute_reply":"2023-08-01T09:17:24.421489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Actual training\n\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nimport gc\nimport os\nimport torch\nimport yaml\nimport pandas as pd\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, TQDMProgressBar\nfrom torch.utils.data import DataLoader\nfrom sklearn.model_selection import KFold\nfrom pytorch_lightning.loggers import CSVLogger\n\ntorch.set_float32_matmul_precision(\"medium\")\n\nwith open(\"config.yaml\", \"r\") as file_obj:\n    config = yaml.safe_load(file_obj)\n\npl.seed_everything(config[\"seed\"])\n\ngc.enable()\n\ncontrails = os.path.join(config[\"data_path\"], \"contrails/\")\ncontrails2 = config[\"data_path2\"]\n# train_path = os.path.join(config[\"data_path\"], \"train_df.csv\")\n# valid_path = os.path.join(config[\"data_path\"], \"valid_df.csv\")\n\n# train_df = pd.read_csv(train_path)\n# valid_df = pd.read_csv(valid_path)\n\n# train_df[\"path\"] = contrails + train_df[\"record_id\"].astype(str) + \".npy\"\n# valid_df[\"path\"] = contrails + valid_df[\"record_id\"].astype(str) + \".npy\"\n\ncon_df = pd.read_csv('/kaggle/input/contrails-csv/Contrail.csv')\ncon_df2 = pd.read_csv('/kaggle/input/contrails-csv/Contrail_2.csv')\n\ncon_df[\"path\"] = contrails + con_df[\"record_id\"].astype(str) + \".npy\"\ncon_df2[\"path\"] = contrails2 + con_df2[\"record_id\"].astype(str) + \".npy\"\n\ndf = pd.concat([con_df, con_df2]).reset_index()\n# df[\"kfold\"] = con_df[\"split\"]\n\n# Fold = KFold(shuffle=True, **config[\"folds\"])\n# for n, (trn_index, val_index) in enumerate(Fold.split(df)):\n#     df.loc[val_index, \"kfold\"] = int(n)\ndf[\"kfold\"] = df[\"split\"].astype(int)\n\nfor fold in config[\"train_folds\"]:\n    print(f\"\\n###### Fold {fold}\")\n    trn_df = df[df.kfold != fold].reset_index(drop=True)\n    vld_df = df[df.kfold == fold].reset_index(drop=True)\n\n    dataset_train = ContrailsDataset(trn_df, config[\"model\"][\"image_size\"], train=False)\n    dataset_validation = ContrailsDataset(vld_df, config[\"model\"][\"image_size\"], train=False)\n\n    data_loader_train = DataLoader(\n        dataset_train,\n        batch_size=config[\"train_bs\"],\n        shuffle=True,\n        num_workers=config[\"workers\"],\n    )\n    data_loader_validation = DataLoader(\n        dataset_validation,\n        batch_size=config[\"valid_bs\"],\n        shuffle=False,\n        num_workers=config[\"workers\"],\n    )\n\n    checkpoint_callback = ModelCheckpoint(\n        save_weights_only=True,\n        monitor=\"val_dice\",\n        dirpath=config[\"output_dir\"],\n        mode=\"max\",\n        filename=f\"model-f{fold}-{{val_dice:.4f}}\",\n        save_top_k=1,\n        verbose=1,\n    )\n\n    progress_bar_callback = TQDMProgressBar(\n        refresh_rate=config[\"progress_bar_refresh_rate\"]\n    )\n\n    early_stop_callback = EarlyStopping(**config[\"early_stop\"])\n\n\n    trainer = pl.Trainer(\n        callbacks=[checkpoint_callback, early_stop_callback, progress_bar_callback],\n        logger=CSVLogger(save_dir=f'logs_f{fold}/'),\n        **config[\"trainer\"],\n    )\n\n    model = LightningModule(config[\"model\"])\n\n    trainer.fit(model, data_loader_train, data_loader_validation)\n\n    del (\n        dataset_train,\n        dataset_validation,\n        data_loader_train,\n        data_loader_validation,\n        model,\n        trainer,\n        checkpoint_callback,\n        progress_bar_callback,\n        early_stop_callback,\n    )\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-08-01T09:17:24.427013Z","iopub.execute_input":"2023-08-01T09:17:24.429911Z","iopub.status.idle":"2023-08-01T09:17:59.082039Z","shell.execute_reply.started":"2023-08-01T09:17:24.429877Z","shell.execute_reply":"2023-08-01T09:17:59.080631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sn\nimport matplotlib.pyplot as plt\n\nfor fold in config[\"train_folds\"]:\n    metrics = pd.read_csv(f\"/kaggle/working/logs_f{fold}/lightning_logs/version_0/metrics.csv\")\n    del metrics[\"step\"]\n    del metrics[\"lr\"]\n    del metrics[\"train_loss_step\"]\n    metrics.set_index(\"epoch\", inplace=True)\n    g = sn.relplot(data=metrics, kind=\"line\")\n    plt.title(f\"Fold {fold}\")\n    plt.gcf().set_size_inches(15, 5)\n    plt.grid()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-01T09:17:59.083960Z","iopub.execute_input":"2023-08-01T09:17:59.084627Z","iopub.status.idle":"2023-08-01T09:17:59.848102Z","shell.execute_reply.started":"2023-08-01T09:17:59.084587Z","shell.execute_reply":"2023-08-01T09:17:59.846625Z"},"trusted":true},"execution_count":null,"outputs":[]}]}